# Test Persistence and Re-run Capability Implementation

## Overview

This document describes the implementation of test persistence and re-run capability for TestGPT, allowing AI-generated tests to be stored, managed, and re-executed with different configurations.

## Architecture

### Backend (Python + FastAPI)

```
backend/
├── database.py           # SQLAlchemy models and database setup
├── schemas.py            # Pydantic request/response models
├── crud.py              # Database CRUD operations
├── test_runner_service.py  # Test execution with config support
├── seed_data.py         # Default configuration templates
└── api/
    └── main.py          # FastAPI REST API endpoints
```

### Frontend (Next.js + TypeScript)

```
frontend/
├── lib/
│   ├── api/
│   │   └── client.ts    # API client for backend communication
│   └── db/
│       └── testgpt.db   # Shared SQLite database
└── app/(dashboard)/
    └── test-library/    # Test management UI
        ├── page.tsx     # Test library list view
        ├── [testId]/
        │   ├── page.tsx # Test details view
        │   └── run/
        │       └── page.tsx  # Test execution config UI
```

## Database Schema

### Core Tables

#### `test_suites`
Stores reusable test scenarios generated by AI.

| Column | Type | Description |
|--------|------|-------------|
| id | String (PK) | Unique test suite ID |
| name | String | Test suite name |
| description | Text | Optional description |
| prompt | Text | Original prompt that generated the test |
| target_url | String | URL being tested |
| test_steps | JSON | Array of test steps (Playwright actions) |
| created_at | DateTime | Creation timestamp |
| last_run | DateTime | Last execution timestamp |
| created_by | String | User/source that created it |
| source_type | String | slack_trigger, github_pr, manual |
| tags | JSON | Array of tags for categorization |

#### `configuration_templates`
Reusable configuration presets for test execution.

| Column | Type | Description |
|--------|------|-------------|
| id | String (PK) | Unique config ID |
| name | String | Configuration name |
| description | Text | Optional description |
| browsers | JSON | Array of browser names |
| viewports | JSON | Array of viewport configs |
| network_modes | JSON | Array of network conditions |
| screenshot_on_failure | Boolean | Capture screenshots on failure |
| video_recording | Boolean | Record video of execution |
| parallel_execution | Boolean | Run tests in parallel |
| max_workers | Integer | Max parallel workers |
| default_timeout | Integer | Default timeout in ms |

#### `test_executions_v2`
Records of test runs with results.

| Column | Type | Description |
|--------|------|-------------|
| id | String (PK) | Unique execution ID |
| test_suite_id | String (FK) | Reference to test suite |
| config_id | String (FK) | Reference to config template |
| status | String | pending, running, passed, failed |
| started_at | DateTime | Start time |
| completed_at | DateTime | Completion time |
| execution_time_ms | Integer | Duration in milliseconds |
| execution_logs | JSON | Detailed execution logs |
| screenshots | JSON | Array of screenshot paths |
| video_url | String | Video recording URL |
| error_details | Text | Error message if failed |
| browser | String | Browser used |
| viewport_width | Integer | Viewport width |
| viewport_height | Integer | Viewport height |
| network_mode | String | Network condition used |
| triggered_by | String | slack, manual, github |
| triggered_by_user | String | Username |

## API Endpoints

### Test Suite Management

**POST /api/tests**
Create a new test suite
```json
{
  "name": "Homepage Signup Flow",
  "description": "Test user signup from homepage",
  "prompt": "Test signup flow on example.com",
  "target_url": "https://example.com",
  "test_steps": [
    {
      "step_number": 1,
      "action": "navigate",
      "target": "https://example.com",
      "expected_outcome": "Page loads successfully"
    }
  ],
  "tags": ["signup", "critical"]
}
```

**GET /api/tests**
List all test suites (with filtering)
Query params: `skip`, `limit`, `tags`, `search`

**GET /api/tests/{test_id}**
Get specific test suite details

**PUT /api/tests/{test_id}**
Update test suite

**DELETE /api/tests/{test_id}**
Delete test suite

### Configuration Templates

**POST /api/configs**
Create configuration template

**GET /api/configs**
List all configuration templates

**GET /api/configs/{config_id}**
Get specific configuration

**PUT /api/configs/{config_id}**
Update configuration

**DELETE /api/configs/{config_id}**
Delete configuration

### Test Execution

**POST /api/tests/{test_id}/run**
Execute a test with configuration
```json
{
  "config_id": "config-abc123",
  "browser": "chrome",
  "viewport_width": 1920,
  "viewport_height": 1080,
  "network_mode": "online",
  "triggered_by": "manual"
}
```

**GET /api/executions**
List all executions (with filtering)

**GET /api/executions/{execution_id}**
Get execution details and results

**GET /api/tests/{test_id}/history**
Get execution history for a test

**POST /api/tests/batch/run**
Run multiple tests with same config
```json
{
  "test_suite_ids": ["suite-1", "suite-2"],
  "config_id": "config-abc123",
  "triggered_by": "manual"
}
```

### Statistics

**GET /api/statistics**
Get overall testing statistics

## Frontend Components

### Test Library Page
`/test-library`

- Grid/list view of all test suites
- Search and filter by tags
- Quick actions: View, Run, Edit, Delete
- Shows last run time and status

### Test Details Page
`/test-library/{testId}`

- Full test suite details
- Test steps breakdown
- Execution history
- Statistics (total runs, pass rate)
- Recent executions list

### Test Runner Page
`/test-library/{testId}/run`

- Configuration preset selector
- Browser selection (Chrome, Firefox, Safari, Edge)
- Viewport selection (Mobile, Tablet, Desktop)
- Network condition selection
- Run test button
- Redirects to execution page when started

### Test Execution Page
`/test-executions/{executionId}`

- Live execution status
- Step-by-step progress
- Screenshots and logs
- Final results and timing

## Configuration File Support

### testgpt.config.yml

```yaml
test_suites:
  regression:
    browsers: [chrome, firefox, safari]
    viewports:
      - mobile: { width: 375, height: 667 }
      - desktop: { width: 1920, height: 1080 }
    network_modes: [online, slow3g]

  smoke:
    browsers: [chrome]
    viewports:
      - desktop: { width: 1920, height: 1080 }
    network_modes: [online]

default_timeout: 30000
screenshot_on_failure: true
video_recording: false
parallel_execution: true
max_workers: 4
```

## Usage Examples

### 1. Save a Test from Slack

When a test is created via Slack:
```python
# In testgpt_engine.py
from backend.crud import create_test_suite
from backend.schemas import TestSuiteCreate

suite = TestSuiteCreate(
    name="Slack Test: Homepage",
    prompt=slack_message,
    target_url="https://example.com",
    test_steps=generated_steps,
    created_by=slack_user_id,
    source_type="slack_trigger",
    tags=["slack", "automated"]
)

db_suite = create_test_suite(db, suite)
```

### 2. Re-run a Test with Different Config

Via frontend or API:
```typescript
// Frontend: Run test button clicked
const execution = await apiClient.runTest("suite-abc123", {
  config_id: "config-regression",
  triggered_by: "manual"
});

// Redirects to /test-executions/{execution.id}
```

### 3. Batch Run Tests

```typescript
// Run multiple tests with same config
const batch = await apiClient.runBatchTests({
  test_suite_ids: ["suite-1", "suite-2", "suite-3"],
  config_id: "config-smoke",
  triggered_by: "manual"
});

// Returns batch_id and execution_ids
```

## Deterministic Execution Features

### 1. Fixed Configuration
- Browser, viewport, and network settings locked per execution
- Reproducible across runs

### 2. Explicit Waits
- Use `waitForSelector` instead of arbitrary `sleep`
- Timeout-based waiting with clear expectations

### 3. Environment Variables
- Test credentials loaded from `.env`
- Consistent data across executions

### 4. Test Data Generation
- Fixed random seeds for reproducible data
- Configurable in `testgpt.config.yml`

## Setup Instructions

### 1. Install Dependencies

```bash
pip install -r requirements.txt
```

### 2. Initialize Database

```bash
python backend/seed_data.py
```

This creates:
- Database tables
- 6 default configuration templates (Regression, Smoke, Mobile, Cross-Browser, Performance, Quick Debug)

### 3. Start Backend API

```bash
bash start_backend.sh
```

Or manually:
```bash
cd backend/api && python main.py
```

API will be available at:
- http://localhost:8000
- Documentation: http://localhost:8000/docs

### 4. Start Frontend

```bash
cd frontend
npm install
npm run dev
```

Frontend will be available at:
- http://localhost:3000

### 5. Migrate Existing JSON Tests (Optional)

```bash
curl -X POST http://localhost:8000/api/migrate/json-to-db
```

This migrates tests from `testgpt_data/scenarios/` to the database.

## Integration with Existing TestGPT

### Slack Integration

Modify `testgpt_engine.py` to save tests:

```python
from backend.database import SessionLocal
from backend.crud import create_test_suite
from backend.schemas import TestSuiteCreate, TestStepSchema

# After test is generated
db = SessionLocal()

# Convert scenario to test suite
test_steps = [
    TestStepSchema(
        step_number=i,
        action=step.action.value,
        target=step.target,
        expected_outcome=step.expected_outcome,
        timeout_seconds=step.timeout_seconds
    )
    for i, step in enumerate(scenario.steps)
]

suite = TestSuiteCreate(
    name=scenario.scenario_name,
    prompt=user_message,
    target_url=scenario.target_url,
    test_steps=test_steps,
    created_by=slack_user_id,
    source_type="slack_trigger",
    tags=["slack"]
)

db_suite = create_test_suite(db, suite)
db.close()
```

### Test Re-run from Slack

```python
# In slack command handler
if message.startswith("re-run"):
    # Extract test name
    test_name = message.replace("re-run", "").strip()

    # Find test suite
    suites = crud.get_test_suites(db, search=test_name, limit=1)

    if suites:
        # Create execution
        execution = crud.create_test_execution(db, TestExecutionCreate(
            test_suite_id=suites[0].id,
            triggered_by="slack",
            triggered_by_user=slack_user_id
        ))

        # Run test
        await runner.execute_test_with_config(
            execution.id,
            test_suite_dict,
            config_dict
        )
```

## Testing

### Test Backend API

```bash
# Start server
python backend/api/main.py

# In another terminal, test endpoints
curl http://localhost:8000/health

# Create test suite
curl -X POST http://localhost:8000/api/tests \
  -H "Content-Type: application/json" \
  -d '{
    "name": "Test Suite",
    "prompt": "Test homepage",
    "target_url": "https://example.com",
    "test_steps": []
  }'

# List test suites
curl http://localhost:8000/api/tests

# List configs
curl http://localhost:8000/api/configs
```

### Test Frontend

```bash
cd frontend
npm run dev

# Open http://localhost:3000/test-library
# Should see test library page
# Click "Create New Test" to test form
```

## Future Enhancements

### 1. Test Versioning
- Track changes to test suites over time
- Compare versions and rollback if needed

### 2. Scheduled Runs
- Cron-like scheduling for recurring tests
- Email/Slack notifications on completion

### 3. Test Dependencies
- Define test execution order
- Setup/teardown scripts

### 4. Environment Management
- Multiple environments (dev, staging, prod)
- Environment-specific configurations

### 5. API Contract Testing
- Integration with OpenAPI specs
- Automatic API endpoint validation

### 6. Visual Regression Testing
- Screenshot comparison across runs
- Highlight visual differences

### 7. Performance Benchmarking
- Track load times over time
- Alert on performance degradation

## Troubleshooting

### Backend won't start
- Check Python version (3.8+)
- Verify all dependencies installed: `pip install -r requirements.txt`
- Check database path is writable

### Frontend can't connect to backend
- Verify backend is running on port 8000
- Check CORS settings in `backend/api/main.py`
- Set `NEXT_PUBLIC_API_URL` env var if needed

### Tests not executing
- Check MCP server is available
- Verify Playwright is installed: `npx playwright install`
- Check test_executor.py logs

### Database errors
- Delete database and reinitialize: `rm frontend/lib/db/testgpt.db && python backend/seed_data.py`
- Check SQLAlchemy version compatibility

## Security Considerations

### 1. API Authentication
Currently no auth - add JWT or API keys for production:
```python
from fastapi.security import HTTPBearer

security = HTTPBearer()

@app.get("/api/tests")
def list_tests(token: str = Depends(security)):
    # Verify token
    pass
```

### 2. Input Validation
- All inputs validated via Pydantic schemas
- SQL injection prevented by SQLAlchemy ORM
- XSS prevention in frontend via React

### 3. Environment Variables
- Keep `.env` file out of git
- Use secrets manager in production
- Rotate credentials regularly

## Performance Optimization

### 1. Database Indexing
Add indexes for common queries:
```python
Index('idx_test_suites_created_at', TestSuite.created_at)
Index('idx_executions_status', TestExecution.status)
```

### 2. Caching
- Cache configuration templates
- Cache test suite lists
- Use Redis for production

### 3. Async Execution
- Tests run asynchronously via test_runner_service.py
- Background worker processes pending executions
- Queue system for large batches

## Monitoring and Observability

### Logging
- API access logs
- Test execution logs
- Error tracking with stack traces

### Metrics
- Test execution counts
- Average execution time
- Pass/fail rates
- Most-run tests

### Alerts
- Failed test notifications
- System health checks
- Database capacity warnings

---

## Summary

This implementation provides:

✅ **Full test persistence** - AI-generated tests saved to database
✅ **Re-run capability** - Execute saved tests with different configs
✅ **Configuration templates** - Reusable preset configurations
✅ **REST API** - FastAPI backend for all operations
✅ **Modern UI** - Next.js frontend for test management
✅ **Deterministic execution** - Reproducible test results
✅ **Execution history** - Track all test runs
✅ **Batch execution** - Run multiple tests at once
✅ **Integration ready** - Works with existing Slack/GitHub flow

**Status**: Implementation complete and ready for testing.
